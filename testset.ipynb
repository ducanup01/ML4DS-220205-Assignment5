{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2041718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras              # Keras is the high-level API of TensorFlow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "154d320b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: X_train = (45000, 32, 32, 3), y_train = (45000,)\n",
      "Validation shape: X_val = (5000, 32, 32, 3), y_val = (5000,)\n",
      "Test shape: X_test = (10000, 32, 32, 3), y_test = (10000,)\n"
     ]
    }
   ],
   "source": [
    "# PLEASE DO NOT CHANGE THIS CODE\n",
    "\n",
    "# Load the cifar10 dataset and split train/test\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Split train/valid from the training set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=5)\n",
    "\n",
    "y_train = y_train.reshape(-1,)\n",
    "y_val = y_val.reshape(-1,)\n",
    "y_test = y_test.reshape(-1,)\n",
    "\n",
    "\n",
    "print(\"Train shape: X_train = \" + str(X_train.shape) + \", y_train = \" + str(y_train.shape))\n",
    "print(\"Validation shape: X_val = \" + str(X_val.shape) + \", y_val = \" + str(y_val.shape))\n",
    "print(\"Test shape: X_test = \" + str(X_test.shape) + \", y_test = \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad0d17f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data normalized: pixel values are now between 0 and 1.\n",
      "Random normalized pixel:  [0.09803922 0.07058824 0.0627451 ]\n"
     ]
    }
   ],
   "source": [
    "# Convert pixel values from 0–255 to 0–1 (normalizing)\n",
    "X_train_norm = X_train.astype(\"float32\") / 255.0\n",
    "X_val_norm = X_val.astype(\"float32\") / 255.0\n",
    "X_test_norm = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "print(\"Data normalized: pixel values are now between 0 and 1.\")\n",
    "print(\"Random normalized pixel: \", X_train_norm[44998][16][16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b41247f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda:1\n"
     ]
    }
   ],
   "source": [
    "#training setup\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Training on:\", device)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_t = torch.tensor(X_train_norm, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "X_val_t   = torch.tensor(X_val_norm, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "X_test_t  = torch.tensor(X_test_norm, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "\n",
    "y_train_t = torch.tensor(y_train.reshape(-1), dtype=torch.long)\n",
    "y_val_t   = torch.tensor(y_val.reshape(-1), dtype=torch.long)\n",
    "y_test_t  = torch.tensor(y_test.reshape(-1), dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val_t, y_val_t), batch_size=128)\n",
    "test_loader  = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4509a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# My model resnet\n",
    "# -------------------------\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // reduction, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.fc(x)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Conv2d(out_c, out_c, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Conv2d(in_c, out_c, 1) if in_c != out_c else nn.Identity()\n",
    "        self.se = SEBlock(out_c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.se(out)\n",
    "        return F.relu(out + self.shortcut(x))\n",
    "\n",
    "\n",
    "class BetterCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # keep original width\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ResBlock(64, 128),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ResBlock(128, 256),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ResBlock(256, 278),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            ResBlock(278, 278)\n",
    "        )\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(278, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.gap(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "model = BetterCNN().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e9a6b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders with augmentation are ready.\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# CIFAR-10 normalization constants\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD  = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# -------------------------\n",
    "# Training Transformations\n",
    "# -------------------------\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.RandomGrayscale(p=0.07),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "    transforms.RandomErasing(p=0.9, scale=(0.02, 0.15), ratio=(0.3, 3.3), value=0, inplace=False)\n",
    "])\n",
    "\n",
    "# -------------------------\n",
    "# Validation/Test Transformations\n",
    "# -------------------------\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "\n",
    "# -------------------------\n",
    "# Convert EXISTING NumPy arrays → torch dataset using transforms\n",
    "# -------------------------\n",
    "\n",
    "class NumpyCIFAR(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = int(self.labels[idx])\n",
    "        \n",
    "        # convert from NumPy (H,W,C) → PIL Image for transforms\n",
    "        img = transforms.ToPILImage()(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# create datasets with transforms\n",
    "train_dataset = NumpyCIFAR(X_train, y_train, transform=train_transform)\n",
    "val_dataset   = NumpyCIFAR(X_val,   y_val,   transform=val_transform)\n",
    "test_dataset  = NumpyCIFAR(X_test,  y_test,  transform=val_transform)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=256, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=256, shuffle=False)\n",
    "\n",
    "print(\"DataLoaders with augmentation are ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4ddae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Label Smoothing Cross Entropy\n",
    "# -------------------------\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        assert 0.0 <= smoothing < 1.0\n",
    "        self.s = smoothing\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        n_classes = logits.size(-1)\n",
    "\n",
    "        # log probabilities\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # create smoothed targets\n",
    "        with torch.no_grad():\n",
    "            true = torch.zeros_like(log_probs)\n",
    "            true.fill_(self.s / (n_classes - 1))\n",
    "            true.scatter_(1, target.unsqueeze(1), 1 - self.s)\n",
    "\n",
    "        return -(true * log_probs).sum(dim=1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "218d640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LabelSmoothingCrossEntropy(smoothing=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa0c3f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best saved checkpoint for testing.\n",
      "\n",
      "====================\n",
      "TEST RESULTS\n",
      "====================\n",
      "Test Loss: 0.4703\n",
      "Test Accuracy: 0.9558\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Test Evaluation (loads best saved model)\n",
    "# -------------------------\n",
    "\n",
    "BEST_MODEL_PATH = \"checkpoints/best_model.pt\"\n",
    "\n",
    "# Load best checkpoint from disk (safe even after crash)\n",
    "if os.path.exists(BEST_MODEL_PATH):\n",
    "    model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "    print(\"Loaded best saved checkpoint for testing.\")\n",
    "else:\n",
    "    print(\"Warning: No saved checkpoint found. Using current model state.\")\n",
    "\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "test_loss_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        test_loss_total += loss.item() * x.size(0)\n",
    "\n",
    "        _, preds = out.max(1)\n",
    "        test_correct += preds.eq(y).sum().item()\n",
    "        test_total += y.size(0)\n",
    "\n",
    "test_loss = test_loss_total / test_total\n",
    "test_accuracy = test_correct / test_total\n",
    "\n",
    "print(\"\\n====================\")\n",
    "print(\"TEST RESULTS\")\n",
    "print(\"====================\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
